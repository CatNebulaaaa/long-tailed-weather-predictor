# train_moe_spatial.py
import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import torchvision
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd # æ–°å¢ä¾èµ–

from dataset import GPMDataset
from models.swin_unet_moe import SwinUNetMoE
# å¯¼å…¥ V3 æ ‡ç­¾å‡½æ•° (è¯·ç¡®ä¿ models/regime_module.py ä¸­æœ‰æ­¤å‡½æ•°)
# from models.regime_module import generate_pseudo_soft_targets_v3 as generate_pseudo_soft_targets
from models.regime_module import generate_pseudo_soft_targets_v1_plus as generate_pseudo_soft_targets
from models.regime_module import generate_pixel_labels

# ================= é…ç½®åŒº =================
DATA_PATH = '/root/autodl-tmp/gpm_pt_dataset'
STATS_PATH = '/root/autodl-tmp/normalization_stats.json'
SAVE_DIR = '/root/autodl-tmp/checkpoints/hcr_v1_plus_pro_attention_v2.11_fine_tuning' 
BASELINE_PATH = '/root/autodl-tmp/checkpoints/weighted_baseline/best_model.pth' 
resume_path = '/root/autodl-tmp/checkpoints/hcr_v1_plus_pro_attention_v2.11/best_extreme_model.pth' 
LOG_DIR = 'runs/hcr_v1_plus_pro_attention_v2.11_fine_tuning'

BATCH_SIZE = 256
LEARNING_RATE = 2e-6
EPOCHS = 70
NUM_WORKERS = 12
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
PAST_FRAMES = 3

# === Loss æƒé‡ ===
# FINAL_LOSS_WEIGHT = 0.1       # é™! ç»™åˆ†ç±» Loss è®©è·¯
# ROUTER_LOSS_WEIGHT = 0.5 
# SPECIALIZED_LOSS_WEIGHT = 1.0 
# GDL_LOSS_WEIGHT = 1.0         # é™! 2.0 æœ‰ç‚¹å–§å®¾å¤ºä¸»
# FINAL_EX_WEIGHT = 0.5 
# DICE_WEIGHT = 1.0             # ä¿æŒä¸»åŠ›
# ENTROPY_WEIGHT = 0.5          # å‡! (ä»0.2æåˆ°0.5) å¼ºè¿« Router ç«™é˜Ÿ
# FOCAL_WEIGHT = 10.0           # æ–°å¢! Focalæ•°å€¼å°ï¼Œç»™å¤§æƒé‡
# === ä¿®æ­£åçš„å»ºè®®æƒé‡ ===
# åœ¨é…ç½®åŒºä¿®æ”¹æƒé‡
# === ä¿®æ­£åçš„å»ºè®®æƒé‡ (å®Œå…¨æ›¿æ¢ä½ ä»£ç ä¸­å¯¹åº”çš„éƒ¨åˆ†) ===
FINAL_LOSS_WEIGHT = 5.0        
ROUTER_LOSS_WEIGHT = 20.0      # ä¿æŒ
DICE_WEIGHT = 5.0              # ç»´æŒ Dice ä¼˜åŒ–å½¢çŠ¶
FINAL_EX_WEIGHT = 5.0         # ä¿æŒ 40.0ï¼Œè¿™æ˜¯æŠŠ RMSE ä» 6.5 æ‰“ä¸‹æ¥çš„å…³é”®
BIAS_PENALTY_WEIGHT = 100.0    
SPECIALIZED_LOSS_WEIGHT = 10.0 
GDL_LOSS_WEIGHT = 20.0         
FFT_WEIGHT = 0.0002            
ENTROPY_WEIGHT = 0.0           # ä¸åŠ 
FOCAL_WEIGHT = 0.0     

# å»ºè®®ï¼šèƒŒæ™¯è®¾ä½ï¼Œæš´é›¨è®¾é«˜
lambda_0 = 2.0  # èƒŒæ™¯
lambda_1 = 1.0  # å±‚çŠ¶äº‘
lambda_2 = 10.0 # æš´é›¨ (æå€¼æœ€éš¾å­¦ï¼Œç»™æœ€å¤§æƒé‡)

GRAD_CLIP = 1.0 
WARMUP_EPOCHS = 0

# === è¯„ä¼°æŒ‡æ ‡ ===
EXTREME_BUCKET = (10.0, 200.0)
CSI_THRESHOLD = 10.0 # è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ CSI çš„é˜ˆå€¼
# æœ€ç»ˆæµ‹è¯•çš„åˆ†æ¡¶å’Œé˜ˆå€¼
FINAL_BUCKETS = [(0, 0.1), (0.1, 1.0), (1.0, 5.0), (5.0, 10.0), (10.0, 200.0)]
FINAL_THRESHOLDS = [0.1, 1.0, 5.0, 10.0, 20.0]

# åŠ è½½é¢„è®¡ç®—çš„ IB æƒé‡
ib_data = torch.load('ib_weights.pt')
# å¿…é¡»è½¬åˆ° GPU
IB_BIN_EDGES = ib_data['bin_edges'].to(DEVICE)
IB_WEIGHTS = ib_data['weights'].to(DEVICE)
# ==========================================
def ib_weighted_mse(pred_norm, target_norm, target_raw, mask=None):
    """
    ã€è½¯åŒ–ç‰ˆ IB Lossã€‘
    å¯¹æŸ¥è¡¨å¾—åˆ°çš„æƒé‡å¼€æ ¹å·ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
    """
    # 1. æŸ¥è¡¨ (ä¿æŒä¸å˜)
    indices = torch.bucketize(target_raw, IB_BIN_EDGES, right=False) - 1
    indices = torch.clamp(indices, min=0, max=len(IB_WEIGHTS)-1)
    weights_map = IB_WEIGHTS[indices]
    
    # 2. ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šæƒé‡è½¯åŒ– (Square Root)
    # åŸå§‹æƒé‡èŒƒå›´: 1 ~ 100
    # è½¯åŒ–åèŒƒå›´: 1 ~ 10
    weights_map = torch.sqrt(weights_map)
    
    # 3. è®¡ç®—åŠ æƒ MSE (ä¿æŒä¸å˜)
    diff_sq = (pred_norm - target_norm) ** 2
    weighted_diff = diff_sq * weights_map
    
    if mask is not None:
        loss = (weighted_diff * mask).sum() / (mask.sum() + 1e-6)
    else:
        loss = weighted_diff.mean()
        
    return loss

def asymmetric_weighted_mse(pred, target, under_penalty=20.0):
    """
    éå¯¹ç§°åŠ æƒ MSEï¼š
    1. åŠ¨æ€æƒé‡ï¼šæ•°å€¼è¶Šå¤§ï¼ŒåŸºç¡€æƒé‡è¶Šå¤§ (é’ˆå¯¹ 50mm)
    2. éå¯¹ç§°æƒ©ç½šï¼šå¦‚æœé¢„æµ‹å€¼ < çœŸå€¼ (æ¼æŠ¥)ï¼Œæƒ©ç½šç¿»å€ï¼
    """
    diff = pred - target
    
    # 1. åŸºç¡€æƒé‡ï¼šé’ˆå¯¹å¤§å€¼åŒºåŸŸ (åŒä¹‹å‰)
    # 50mm -> åŸºç¡€æƒé‡ 25
    base_weight = torch.clamp((target / 10.0) ** 2, min=1.0)
    
    # 2. éå¯¹ç§°æƒ©ç½š (Asymmetric Penalty)
    # æ‰¾å‡ºâ€œæ¼æŠ¥â€çš„åƒç´  (pred < target)
    under_prediction_mask = (diff < 0).float()
    
    # å¦‚æœæ¼æŠ¥ï¼Œæƒé‡å†ä¹˜ä»¥ under_penalty (æ¯”å¦‚ 20å€)
    # å¦‚æœè¿‡æŠ¥ï¼Œæƒé‡ä¿æŒä¸º 1
    # é€»è¾‘ï¼šå®å¯è¿‡æŠ¥ï¼Œä¸å¯æ¼æŠ¥ï¼
    asymmetric_w = 1.0 + (under_penalty - 1.0) * under_prediction_mask
    
    # æœ€ç»ˆæƒé‡ = åŸºç¡€æƒé‡ * éå¯¹ç§°æƒé‡
    final_weight = base_weight * asymmetric_w
    
    loss = (diff ** 2 * final_weight).mean()
    return loss


def make_weighted_sampler(dataset):
    print("âš–ï¸ æ­£åœ¨æ‰«ææ•°æ®é›†ä»¥è®¡ç®—é‡‡æ ·æƒé‡ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    weights = []
    
    # ä¸ºäº†åŠ å¿«é€Ÿåº¦ï¼Œå¦‚æœä½ æœ‰é¢„å…ˆå­˜å¥½çš„ metadata æœ€å¥½
    # è¿™é‡Œæˆ‘ä»¬éå† datasetã€‚å¦‚æœå¤ªæ…¢ï¼Œå¯ä»¥å°è¯•åªéå†ä¸€éƒ¨åˆ†
    for i in tqdm(range(len(dataset))):
        # å‡è®¾ dataset[i] è¿”å› (input, norm, raw, label)
        # æˆ‘ä»¬åªéœ€è¦ raw target (index 2)
        try:
            # å¿«é€Ÿè¯»å–ï¼šåªè¯» tensorï¼Œä¸è¿›è¡Œ transform (å¦‚æœ dataset æ”¯æŒ)
            # è¿™é‡Œç›´æ¥è°ƒç”¨ getitem å¯èƒ½ä¼šæ…¢ï¼Œä½†æœ€ç¨³å¦¥
            _, _, target_raw, _ = dataset[i]
            max_val = target_raw.max().item()
        except:
            max_val = 0.0
            
        # ğŸ”¥ æƒé‡åˆ†é…ç­–ç•¥ï¼šè®©æš´é›¨å‡ºç°çš„æ¦‚ç‡ç¿»å€å†ç¿»å€
        if max_val >= 50.0:
            w = 20.0  # è¶…çº§æš´é›¨ï¼š20å€å…³æ³¨åº¦ï¼
        elif max_val >= 30.0:
            w = 10.0  # æš´é›¨
        elif max_val >= 10.0:
            w = 3.0   # å¤§é›¨
        elif max_val >= 1.0:
            w = 1.0   # æ™®é€šé›¨
        else:
            w = 0.6   # æ— é›¨/æ¯›æ¯›é›¨ï¼šå¤§å¹…é™æƒï¼Œå°‘çœ‹ç‚¹
            
        weights.append(w)
        
    weights = torch.tensor(weights).double()
    # Replacement=True æ˜¯å¿…é¡»çš„ï¼Œå…è®¸åŒä¸€ä¸ªæš´é›¨æ ·æœ¬åœ¨ä¸€ä¸ª Batch é‡Œå‡ºç°å¤šæ¬¡
    sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights), replacement=True)
    return sampler

def fft_loss(pred, target):
    """
    é«˜é¢‘åŠ æƒå¢å¼ºç‰ˆ FFT Loss
    ä¸“é—¨è§£å†³ PSD å°¾éƒ¨æ‰è½å’Œç”»é¢æ¨¡ç³Šé—®é¢˜
    """
    pred_fft = torch.fft.rfft2(pred)
    target_fft = torch.fft.rfft2(target)
    diff = torch.abs(pred_fft - target_fft)

    B, C, H, W = pred.shape
    u = torch.fft.fftfreq(H).to(pred.device)
    v = torch.fft.rfftfreq(W).to(pred.device)
    u_grid, v_grid = torch.meshgrid(u, v, indexing='ij')
    dist = torch.sqrt(u_grid**2 + v_grid**2)
    
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šä¸å†æ˜¯çº¿æ€§ distï¼Œè€Œæ˜¯ç»™é«˜é¢‘éƒ¨åˆ†æŒ‡æ•°çº§çš„å‹åŠ›
    # åªæœ‰å½“ dist > 0.4 (å³è¿›å…¥ PSD æ‰è½çš„é«˜é¢‘åŒº) æ—¶ï¼Œæƒé‡æ‰ä¼šé™¡å¢
    # è¿™ä¼šå¼ºè¿«æ¨¡å‹å»å…³æ³¨é‚£äº›ä¸¢å¤±çš„é”åˆ©çº¹ç†
    weight = 1.0 + torch.pow(dist * 10.0, 3) 
    
    return (diff * weight).mean()
    
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        inputs = inputs.flatten()
        targets = targets.flatten()
        bce = F.binary_cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-bce)
        loss = self.alpha * (1 - pt) ** self.gamma * bce
        if self.reduction == 'mean': return loss.mean()
        else: return loss.sum()

class SoftDiceLoss(nn.Module):
    def __init__(self, smooth=1.0):
        super(SoftDiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, logits, targets):
        # logits: æ¨¡å‹çš„åŸå§‹è¾“å‡º (ç»è¿‡åå½’ä¸€åŒ–åçš„ raw precipitation)
        # targets: äºŒå€¼åŒ–çš„çœŸå€¼ (0 æˆ– 1)
        
        # ä¸ºäº†å¯å¯¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ Sigmoid è¿‘ä¼¼é˜¶è·ƒå‡½æ•°ï¼Œæˆ–è€…ç›´æ¥å¤„ç†æ¦‚ç‡
        # è¿™é‡Œæˆ‘ä»¬å‡è®¾è¾“å…¥å·²ç»æ˜¯ç»è¿‡é˜ˆå€¼å¤„ç†çš„â€œæ¦‚ç‡ä¼¼ç„¶â€
        
        # å±•å¹³
        if logits.dim() > 2:
            logits = logits.view(logits.size(0), -1)
            targets = targets.view(targets.size(0), -1)
        
        intersection = (logits * targets).sum(dim=1)
        union = logits.sum(dim=1) + targets.sum(dim=1)
        
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        return 1 - dice.mean()

# è¾…åŠ©å‡½æ•°ï¼šå°†è¿ç»­çš„é™æ°´æ•°å€¼è½¬åŒ–ä¸ºâ€œæ˜¯å¼ºé™æ°´çš„æ¦‚ç‡â€
def get_heavy_rain_probability(precip_raw, threshold=10.0, temperature=5.0):
    # ä½¿ç”¨å¸¦æ¸©åº¦çš„ Sigmoid æ¥æ¨¡æ‹Ÿä» 0 åˆ° 1 çš„ç¡¬æˆªæ–­
    # å½“ precip_raw = 10.0 æ—¶ï¼Œå€¼ä¸º 0.5
    # temperature è¶Šå°ï¼Œè½¬æ¢è¶Šé™¡å³­ï¼ˆè¶Šæ¥è¿‘ 0/1ï¼‰
    return torch.sigmoid((precip_raw - threshold) * temperature)

class EntropyLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        # x: softmax åçš„æ¦‚ç‡ [B, K, H, W]
        b = x * torch.log(x + 1e-8)
        b = -1.0 * b.sum(dim=1)
        return b.mean()

def denormalize(tensor, mean, std):
    x = tensor * std + mean
    x = torch.expm1(x)
    return torch.clamp(x, min=0.0)

def colorize(tensor, cmap_name='jet', vmin=0, vmax=20):
    if tensor.dim() == 4: tensor = tensor.squeeze(0)
    if tensor.dim() == 2: tensor = tensor.unsqueeze(0)
    tensor_norm = np.clip((tensor.detach().cpu().numpy().squeeze() - vmin) / (vmax - vmin), 0, 1)
    cmap = matplotlib.colormaps[cmap_name]
    colored_array = cmap(tensor_norm)
    return torch.from_numpy(colored_array[:, :, :3]).permute(2, 0, 1).float()

class GDLLoss(nn.Module):
    def __init__(self):
        super(GDLLoss, self).__init__()
    def forward(self, pred, target):
        pred_dx = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])
        target_dx = torch.abs(target[:, :, :, 1:] - target[:, :, :, :-1])
        pred_dy = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])
        target_dy = torch.abs(target[:, :, 1:, :] - target[:, :, :-1, :])
        return torch.mean(torch.abs(pred_dx - target_dx)) + torch.mean(torch.abs(pred_dy - target_dy))


def compute_expert_loss(pred, target, mask, weight_map=None, delta=1.0, gdl_coeff=0.1):
    """
    ç»Ÿä¸€æŸå¤±å‡½æ•°ï¼šWeighted Huber Loss + GDL
    delta: æ§åˆ¶ MSE å’Œ L1 çš„åˆ‡æ¢ç‚¹ã€‚
           delta è¶Šå¤§ -> è¶Šåƒ MSE (åˆ©äºé™ RMSE)
           delta è¶Šå° -> è¶Šåƒ L1  (åˆ©äºä¿è¾¹ç¼˜/CSI)
    """
    # 1. è®¡ç®— Masked Huber Loss
    # reduction='none' è®©æˆ‘ä»¬èƒ½åº”ç”¨ mask
    loss_huber_raw = F.huber_loss(pred, target, reduction='none', delta=delta)
    
    # åº”ç”¨æƒé‡ (Weight Map) å’Œ Mask
    if weight_map is not None:
        loss_main = (loss_huber_raw * weight_map * mask).sum() / (mask.sum() + 1e-6)
    else:
        loss_main = (loss_huber_raw * mask).sum() / (mask.sum() + 1e-6)
    
    # 2. è®¡ç®— GDL Loss (æ¢¯åº¦æŸå¤±)
    # GDL ä¹Ÿæ˜¯ä¸ºäº†ä¿è¾¹ç¼˜ï¼Œæš´é›¨ä¸“å®¶ç»™é«˜ä¸€ç‚¹ï¼Œå±‚çŠ¶äº‘ç»™ä½ä¸€ç‚¹
    # æ³¨æ„ï¼šcriterion_gdl éœ€è¦åœ¨å¤–éƒ¨å®šä¹‰æˆ–è€…è¿™é‡Œå®ä¾‹åŒ–
    # å»ºè®®ç›´æ¥æŠŠ criterion_gdl ä¼ è¿›æ¥ï¼Œæˆ–è€…åœ¨è¿™é‡Œç®€å•ç®—ä¸€ä¸‹
    pred_dx = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])
    target_dx = torch.abs(target[:, :, :, 1:] - target[:, :, :, :-1])
    pred_dy = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])
    target_dy = torch.abs(target[:, :, 1:, :] - target[:, :, :-1, :])
    
    loss_gdl = torch.mean(torch.abs(pred_dx - target_dx) * mask[:, :, :, 1:]) + \
               torch.mean(torch.abs(pred_dy - target_dy) * mask[:, :, 1:, :])
    
    return loss_main + gdl_coeff * loss_gdl

# === å¢å¼ºç‰ˆéªŒè¯å‡½æ•°ï¼šè¿”å› RMSE, ExtremeRMSE, CSI å’Œ å¯è§†åŒ–å¼ é‡ ===
# === å…¨å±€è¶…å‚æ•°è®¾ç½® ===
BG_THRESHOLD = 0.7  # èƒŒæ™¯æŠ‘åˆ¶é˜ˆå€¼ï¼šå¦‚æœ Router è®¤ä¸ºèƒŒæ™¯æ¦‚ç‡ > 0.7ï¼Œå¼ºåˆ¶æ¸…é›¶
CSI_THRESHOLD = 10.0 # ä½ çš„æ ¸å¿ƒè¯„æµ‹é˜ˆå€¼

def validate_epoch_detailed(model, loader, stats, device):
    model.eval()
    total_rmse_sq = 0; count_total = 0
    ext_rmse_sq = 0; count_ext = 0
    
    # æŒ‡æ ‡ç»Ÿè®¡
    hits, misses, false_alarms = 0, 0, 0
    
    max_gt_val = -1
    best_batch_tensors = None
    mean, std = stats['mean'], stats['std']
    
    with torch.no_grad():
        for inputs, targets_norm, targets_raw, _ in loader:
            inputs = inputs.to(device)
            targets_norm = targets_norm.to(device)
            targets_raw = targets_raw.to(device)
            
            # === æ ¸å¿ƒä¿®æ”¹ç‚¹ ===
            # æ¨¡å‹ç°åœ¨è¿”å› 4 ä¸ªå€¼ï¼Œæˆ‘ä»¬åªéœ€è¦ ç¬¬1ä¸ª(é¢„æµ‹) å’Œ ç¬¬3ä¸ª(æ¦‚ç‡)
            # ä½¿ç”¨ _ å¿½ç•¥ä¸éœ€è¦çš„ logits å’Œ expert_stack
            final_pred_norm, _, router_probs, _ = model(inputs)
            
            # === èƒŒæ™¯æŠ‘åˆ¶é€»è¾‘ (ä¿æŒä¸å˜) ===
            # 1. å–å‡ºèƒŒæ™¯ä¸“å®¶æ¦‚ç‡ (Channel 0)
            prob_bg = router_probs[:, 0:1, :, :] 
            # 2. ç”Ÿæˆ Mask: å¦‚æœ Router è®¤ä¸ºèƒŒæ™¯æ¦‚ç‡ > 0.7ï¼Œåˆ™å¼ºåˆ¶å°†é¢„æµ‹ç½®ä¸º 0
            # è¿™èƒ½æœ‰æ•ˆæ¶ˆé™¤å› ä¸º Router çŠ¹è±«ä¸å†³å¯¼è‡´çš„â€œæ–¹å—åº•å™ªâ€
            suppression_mask = (prob_bg < BG_THRESHOLD).float()
            # 3. æ‰§è¡ŒæŠ‘åˆ¶
            final_pred_norm = final_pred_norm * suppression_mask
            # =================
            
            # åå½’ä¸€åŒ–
            preds_raw = denormalize(final_pred_norm, mean, std)
            # ç‰©ç†å¸¸è¯†ï¼šé™æ°´å¼ºåº¦ä½äº 0.1mm/h åœ¨æ°”è±¡è§‚æµ‹ä¸­é€šå¸¸è§†ä¸ºæ— é›¨
            preds_raw[preds_raw < 0.1] = 0.0
            
            # 1. æ•°å€¼æŒ‡æ ‡ (RMSE)
            diff = preds_raw - targets_raw
            mask_valid = targets_raw >= 0
            total_rmse_sq += (diff[mask_valid]**2).sum().item()
            count_total += mask_valid.sum().item()
            
            mask_ext = targets_raw >= 10.0
            if mask_ext.sum() > 0:
                ext_rmse_sq += (diff[mask_ext]**2).sum().item()
                count_ext += mask_ext.sum().item()
            
            # 2. åˆ†ç±»æŒ‡æ ‡ (@CSI_THRESHOLD, é€šå¸¸æ˜¯ 10mm)
            # è¿™é‡Œçš„ preds_raw å·²ç»æ˜¯è¢«æŠ‘åˆ¶è¿‡çš„å¹²å‡€ç»“æœäº†
            pred_bin = preds_raw >= CSI_THRESHOLD
            target_bin = targets_raw >= CSI_THRESHOLD
            
            hits += (pred_bin & target_bin).sum().item()
            misses += (~pred_bin & target_bin).sum().item()
            false_alarms += (pred_bin & ~target_bin).sum().item()
            
            # 3. é‡‡æ ·ç”¨äºå¯è§†åŒ–
            current_max = targets_raw.max().item()
            if current_max > max_gt_val:
                max_gt_val = current_max
                # æ‰¾ä¸€ä¸ªæœ€å¤§å€¼æœ€çŒ›çš„æ ·æœ¬
                batch_max_vals = targets_raw.view(targets_raw.size(0), -1).max(dim=1)[0]
                idx = torch.argmax(batch_max_vals).item()
                
                # ä¿å­˜å¯è§†åŒ–æ•°æ® (inputs, pred, gt, probs)
                best_batch_tensors = (inputs[idx].cpu(), preds_raw[idx].cpu(), targets_raw[idx].cpu(), router_probs[idx].cpu())
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    rmse = np.sqrt(total_rmse_sq / (count_total + 1e-6))
    ext_rmse = np.sqrt(ext_rmse_sq / (count_ext + 1e-6)) if count_ext > 0 else 0
    
    # æ ¸å¿ƒåˆ†ç±»æŒ‡æ ‡å…¬å¼
    eps = 1e-6
    csi = hits / (hits + misses + false_alarms + eps)
    pod = hits / (hits + misses + eps)
    far = false_alarms / (hits + false_alarms + eps)
    bias = (hits + false_alarms) / (hits + misses + eps)
    
    # ä»¥å­—å…¸å½¢å¼è¿”å›ï¼Œæ–¹ä¾¿ä¸»å¾ªç¯è®°å½•
    metrics = {
        'rmse': rmse,
        'ext_rmse': ext_rmse,
        'csi': csi,
        'pod': pod,
        'far': far,
        'bias': bias
    }
    
    return metrics, best_batch_tensors

# === æœ€ç»ˆå…¨é¢æµ‹è¯•å‡½æ•° ===
def final_test_pipeline(model, loader, stats, device, output_dir, prefix='final'):
    model.eval()
    print(f"\nğŸš€ Running Final Detailed Test ({prefix}) with Hybrid Gating...")
    
    all_preds = []
    all_targets = []
    
    mean, std = stats['mean'], stats['std']

    with torch.no_grad():
        for inputs, _, targets_raw, _ in tqdm(loader, desc=f"Collecting {prefix}"):
            inputs = inputs.to(device)
            targets_raw = targets_raw.to(device) 
            
            # === æ ¸å¿ƒä¿®æ”¹ç‚¹ ===
            # è§£åŒ… 4 ä¸ªè¿”å›å€¼
            final_pred_norm, _, router_probs, _ = model(inputs, hard_routing=False)
            
            # === èƒŒæ™¯æŠ‘åˆ¶é€»è¾‘ ===
            prob_bg = router_probs[:, 0:1, :, :] 
            suppression_mask = (prob_bg < BG_THRESHOLD).float()
            final_pred_norm = final_pred_norm * suppression_mask
            # =================
            
            preds_raw = denormalize(final_pred_norm, mean, std)
            # ç‰©ç†å¸¸è¯†ï¼šé™æ°´å¼ºåº¦ä½äº 0.1mm/h åœ¨æ°”è±¡è§‚æµ‹ä¸­é€šå¸¸è§†ä¸ºæ— é›¨
            preds_raw[preds_raw < 0.1] = 0.0
            
            if preds_raw.dim() == 4: preds_raw = preds_raw.squeeze(1)
            if targets_raw.dim() == 4: targets_raw = targets_raw.squeeze(1)
            
            # è½¬åˆ° CPU èŠ‚çœæ˜¾å­˜
            all_preds.append(preds_raw.cpu())
            all_targets.append(targets_raw.cpu())

    # æ‹¼æ¥
    all_preds = torch.cat(all_preds, dim=0)
    all_targets = torch.cat(all_targets, dim=0)

    # 1. è®¡ç®—åŸºç¡€æŒ‡æ ‡ (Standard)
    print(">>> Calculating Standard Metrics (Threshold=10.0)...")
    pred_bin = (all_preds >= 10.0)
    target_bin = (all_targets >= 10.0)
    hits = (pred_bin & target_bin).sum().item()
    misses = (~pred_bin & target_bin).sum().item()
    false_alarms = (pred_bin & ~target_bin).sum().item()
    
    std_csi = hits / (hits + misses + false_alarms + 1e-6)
    std_far = false_alarms / (hits + false_alarms + 1e-6)
    print(f"âœ… Standard CSI (Hybrid Gating): {std_csi:.4f}")
    print(f"âœ… Standard FAR (Hybrid Gating): {std_far:.4f}")

    # 2. æœç´¢æœ€ä¼˜é˜ˆå€¼ (Optional)
    print("\n>>> Searching for Optimal Post-Processing Thresholds...")
    best_t = 10.0
    best_csi = 0.0
    
    # åœ¨ 5.0 åˆ° 25.0 ä¹‹é—´æœç´¢
    search_range = torch.arange(5.0, 25.0, 0.5)
    
    for t in search_range:
        pred_bin = (all_preds >= t)
        # ç›®æ ‡æ°¸è¿œæ˜¯ 10.0ï¼Œæˆ‘ä»¬åœ¨æ‰¾å“ªä¸ªé¢„æµ‹é˜ˆå€¼æœ€åŒ¹é… 10.0 çš„çœŸå€¼
        target_bin = (all_targets >= 10.0) 
        
        hits = (pred_bin & target_bin).sum().item()
        misses = (~pred_bin & target_bin).sum().item()
        false_alarms = (pred_bin & ~target_bin).sum().item()
        csi = hits / (hits + misses + false_alarms + 1e-6)
        
        if csi > best_csi:
            best_csi = csi
            best_t = t.item()
            
    print(f"ğŸŒŸ Optimal Threshold found: {best_t:.2f} (maps to GT 10.0)")
    print(f"ğŸŒŸ Best Calibrated CSI: {best_csi:.4f}")

def main():
    if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)
    writer = SummaryWriter(LOG_DIR)
    with open(STATS_PATH, 'r') as f: stats = json.load(f)

    # Dataset
    train_dataset = GPMDataset(DATA_PATH, 'train')
    val_dataset = GPMDataset(DATA_PATH, 'val')
    # å¢åŠ  Test Dataset
    test_dataset = GPMDataset(DATA_PATH, 'test')
    
    # sampler = make_weighted_sampler(train_dataset)
    
    # ğŸ”¥ 2. ä¿®æ”¹ DataLoader
    # train_loader = DataLoader(
    #     train_dataset, 
    #     batch_size=BATCH_SIZE, 
    #     sampler=sampler,      # <--- æ³¨å…¥çµé­‚
    #     shuffle=False,        # ä½¿ç”¨ sampler æ—¶å¿…é¡»å…³æ‰ shuffle
    #     num_workers=NUM_WORKERS,
    #     pin_memory=True
    # )
    train_loader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,        # æ¢å¤ Shuffle
        num_workers=NUM_WORKERS,
        pin_memory=True
    )
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

    print(f"Initializing SwinUNetMoE...")
    model = SwinUNetMoE(img_size=128, in_chans=PAST_FRAMES, num_regimes=3, window_size=4).to(DEVICE)


    if os.path.exists(resume_path):
        print(f"ğŸ”¥ğŸ”¥ğŸ”¥ [Resume] Loading best checkpoint: {resume_path}")
        checkpoint = torch.load(resume_path, map_location=DEVICE)
        
        # è¿‡æ»¤æ‰å½¢çŠ¶ä¸åŒ¹é…çš„å±‚ (ä¸»è¦æ˜¯ router.up1 å’Œ router_upsampler)
        model_dict = model.state_dict()
        pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict and v.shape == model_dict[k].shape}
        
        # åŠ è½½åŒ¹é…çš„æƒé‡ (Encoder å’Œ Expert ä¸å˜ï¼Œä¿ç•™ä¹‹å‰çš„è®­ç»ƒæˆæœ)
        model.load_state_dict(pretrained_dict, strict=False)
        print(f">>> Partially restored! Router upsampling layers will be re-initialized.")
        # print(">>> ğŸ’‰ RE-INJECTING BIAS for Expert 2 Rescue...")
        # with torch.no_grad():
        #     # è·å– Expert 2 çš„æœ€åä¸€å±‚
        #     layer_2 = model.experts[2].up_final
        #     if isinstance(layer_2, nn.Sequential): layer_2 = layer_2[-1]
            
        #     # ç°åœ¨çš„ bias å¯èƒ½å·²ç»å˜æˆè´Ÿæ•°æˆ–è€…å¾ˆå°äº†
        #     current_bias = layer_2.bias.data.mean().item()
        #     print(f"    Current Exp2 Bias: {current_bias:.4f}")
            
        #     # å¼ºè¡ŒåŠ ç ï¼è®¾ä¸º 3.0 (æ¯”ä¹‹å‰çš„ 1.0 æ›´æ¿€è¿›ï¼Œä½†æ¯” 5.0 å®‰å…¨)
        #     # è¿™ä¼šç»™ Expert 2 ä¸€ä¸ªå·¨å¤§çš„å‘ä¸Šåˆé€Ÿåº¦
        #     layer_2.bias.data.fill_(3.0) 
        #     print(f"    New Exp2 Bias: 3.0")

    # if os.path.exists(BASELINE_PATH):
    #     print(f"ğŸ”¥ğŸ”¥ğŸ”¥ [Strategy: Upcycling] Loading Strong Baseline: {BASELINE_PATH}")
    #     baseline_state = torch.load(BASELINE_PATH, map_location=DEVICE)
    #     model_dict = model.state_dict()
    #     new_state_dict = {}
        
    #     # å…³é”®è¯åŒ¹é…: å°† Baseline Decoder å¤åˆ¶ç»™ 3 ä¸ª Experts
    #     decoder_keywords = ['layers_up', 'concat_back_dim', 'norm_up', 'up_final']
        
    #     for k, v in baseline_state.items():
    #         if any(keyword in k for keyword in decoder_keywords):
    #             for i in range(3): 
    #                 expert_key = f"experts.{i}.{k}"
    #                 if expert_key in model_dict and model_dict[expert_key].shape == v.shape:
    #                     new_state_dict[expert_key] = v
    #         else:
    #             if k in model_dict and model_dict[k].shape == v.shape:
    #                 new_state_dict[k] = v
        
    #     keys = model.load_state_dict(new_state_dict, strict=False)
    #     print(f">>> Weights cloned successfully! Missing keys (Expected only Router): {len(keys.missing_keys)}")

    # print(">>> ğŸ› ï¸ Initializing Router Upsampler...")
    # # å¯¹ router_upsampler è¿›è¡Œåˆå§‹åŒ–ï¼Œä½¿å…¶åˆå§‹çŠ¶æ€æ¥è¿‘äºâ€œå¹³æ»‘æ’å€¼â€ï¼Œé¿å…å†·å¯åŠ¨éœ‡è¡
    # for m in model.router_upsampler.modules():
    #     if isinstance(m, nn.ConvTranspose2d):
    #         nn.init.xavier_normal_(m.weight) # âœ… ä½¿ç”¨ Xavier åˆå§‹åŒ–ï¼Œç®€å•ä¸”æœ‰æ•ˆ
    #         if m.bias is not None:
    #             nn.init.zeros_(m.bias)

    # =========================================================
    # ğŸ”¥ã€æœ€ç»ˆä¿®æ­£æ–¹æ¡ˆã€‘æ¸©å’Œåç½® + ä¸“å®¶å†»ç»“ (Gentle Bias + Freeze)
    # =========================================================
    # print("\n>>> ğŸ§Š STRATEGY: Gentle Bias + Router Warmup...")
    
    # with torch.no_grad():
    #     # --- 1. æ¸©å’Œåç½® (Gentle Bias) ---
    #     # ä¸è¦è®¾å¤ªå¤§ï¼Œé˜²æ­¢ Router å› ä¸ºèƒŒæ™¯è¯¯å·®å¤ªå¤§è€ŒæŠ›å¼ƒ Expert 2
        
    #     # Expert 0 (èƒŒæ™¯): è®¾ä¸º -1.0 (è¶³å¤Ÿå‹åˆ¶å™ªå£°)
    #     layer_0 = model.experts[0].up_final
    #     if isinstance(layer_0, nn.Sequential): layer_0 = layer_0[-1]
    #     layer_0.bias.data.fill_(-1.0) 
    #     print(f"    [Bias] Exp0 set to -1.0")

    #     # Expert 2 (æš´é›¨): è®¾ä¸º +1.0 (èµ·æ­¥ä»·ï¼Œå‘Šè¯‰ Router å®ƒæ˜¯æ­£å€¼å³å¯)
    #     # åªè¦æ¯” Expert 0 (-1.0) å¤§ï¼ŒRouter å°±ä¼šå€¾å‘äºé€‰å®ƒå¤„ç†é™æ°´
    #     layer_2 = model.experts[2].up_final
    #     if isinstance(layer_2, nn.Sequential): layer_2 = layer_2[-1]
    #     layer_2.bias.data.fill_(1.0) 
    #     print(f"    [Bias] Exp2 set to +1.0")
        
    #     # Expert 1 (å±‚çŠ¶): è®¾ä¸º 0.0 (ä¸­é—´æ€)
    #     layer_1 = model.experts[1].up_final
    #     if isinstance(layer_1, nn.Sequential): layer_1 = layer_1[-1]
    #     layer_1.bias.data.fill_(0.0)

    # --- 2. å†»ç»“ä¸“å®¶ (Freeze Experts) ---
    # å…³é”®ä¸€æ­¥ï¼å‰å‡ ä¸ª Epoch ä¸¥ç¦ä¸“å®¶ä¹±åŠ¨ï¼Œåªå‡† Router å­¦ä¹ å¦‚ä½•åˆ†é…ï¼
    # print(">>> ğŸ”’ FREEZING all Experts. Training Router ONLY.")
    # for name, param in model.experts.named_parameters():
    #     param.requires_grad = False
    
    # æ­¤æ—¶ä¼˜åŒ–å™¨é‡Œåº”è¯¥åªæœ‰ Router å’Œ Encoder çš„å‚æ•°
    # =========================================================
    
    print(">>> ğŸ”’ Permanently Freezing Expert 0 (Background) parameters...")
    for param in model.experts[0].parameters():
        param.requires_grad = False
        
    # å®šä¹‰ä¼˜åŒ–å™¨ (æ­¤æ—¶ Expert 0 å·²è¢«æ’é™¤)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    
    criterion_l1 = nn.L1Loss()
    criterion_gdl = GDLLoss()
    criterion_entropy = EntropyLoss()
    criterion_focal = FocalLoss(alpha=0.25, gamma=2.0).to(DEVICE)
    # åœ¨ main å‡½æ•°å¼€å¤´
    criterion_dice = SoftDiceLoss()
    criterion_router = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 15.0], device=DEVICE))

    best_extreme_rmse = float('inf')
    best_csi = 0.0

    for epoch in range(EPOCHS):
        # === åŠ¨æ€è§£å†»é€»è¾‘ ===
        # åœ¨ç¬¬ 3 ä¸ª Epoch (å³ index=3ï¼Œå®é™…ä¸Šæ˜¯ç¬¬4è½®ï¼Œæˆ–è€… index=2 ç¬¬3è½®) è§£å†»
        # å»ºè®®è®¾ä¸º 3ï¼Œç»™ Router è¶³å¤Ÿçš„æ—¶é—´çœ‹æ¸… 4x4 çš„ç‰¹å¾
        # if epoch == 3: 
        #     print("\n" + "="*40)
        #     print(f">>> ğŸ”¥ Epoch {epoch+1}: UNFREEZING EXPERTS! Full Training Starts!")
        #     print("="*40 + "\n")
            
        #     # 1. è§£å†»æ‰€æœ‰å‚æ•°
        #     for param in model.experts.parameters():
        #         param.requires_grad = True
            
        #     # 2. é‡ç½®ä¼˜åŒ–å™¨ (é‡è¦ï¼å¦åˆ™ä¼˜åŒ–å™¨ä¸çŸ¥é“è¦æ›´æ–°æ–°å‚æ•°)
        #     # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é‡æ–°æŠŠæ‰€æœ‰å‚æ•°åŠ è¿›å»
        #     optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
        #     # å¦‚æœæœ‰ scheduler ä¹Ÿè¦é‡ç½®ä¸€ä¸‹æˆ–è€…ä¸åšå¤„ç†(ä¼šå»¶ç»­ä¹‹å‰çš„lr)
        # ====================
        model.train()
        meter = {
            'total': 0, 'router': 0, 'spec': 0, 'gdl': 0, 'final': 0, 'final_ex': 0, 'dice': 0,'bias': 0, 'fft': 0,
            'prob_0': 0, 'prob_1': 0, 'prob_2': 0,
            'out_0': 0, 'out_1': 0, 'out_2': 0
        }
        steps = 0
    
        loop = tqdm(train_loader, desc=f"Ep {epoch+1}/{EPOCHS}", leave=False)
        
        if epoch < WARMUP_EPOCHS:
            warmup_lr = LEARNING_RATE * (epoch + 1) / WARMUP_EPOCHS
            for pg in optimizer.param_groups: pg['lr'] = warmup_lr

        for inputs, targets_norm, targets_raw, region_labels in loop:
            inputs, targets_norm, targets_raw, region_labels = \
                inputs.to(DEVICE), targets_norm.to(DEVICE), targets_raw.to(DEVICE), region_labels.to(DEVICE)
            if targets_norm.dim() == 3: targets_norm = targets_norm.unsqueeze(1)
            
            # Forward
            final_pred_norm, router_logits_128, router_probs, expert_preds_stack = model(inputs)
            
            # --- 1. Router Loss (å…¨é«˜æ¸…é—­ç¯ç‰ˆ) ---
            
            # ğŸ”¥ã€ä¸å†éœ€è¦ F.interpolateã€‘ç›´æ¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆçš„ logits
            # è¿™èƒ½ä¿è¯æ¢¯åº¦ç›´æ¥ä½œç”¨äºåå·ç§¯å±‚ï¼Œè®©å®ƒå­¦ä¼šé”åŒ–è¾¹ç¼˜
            # router_log_probs = F.log_softmax(router_logits_128, dim=1)
            
            # ç”Ÿæˆä¼ªæ ‡ç­¾
            static_targets = generate_pseudo_soft_targets(targets_raw, area_kernel_size=5)
            
            # # è®¡ç®— KL æ•£åº¦
            # log_target = torch.log(static_targets.clamp(min=1e-8))
            # kl_raw = static_targets * (log_target - router_log_probs)
            
            # # æƒé‡ (ä¿æŒä½ çš„é…ç½®)
            # weights = torch.tensor([10.0, 1.0, 5.0], device=DEVICE).view(1, 3, 1, 1)
            # loss_router = (kl_raw * weights).sum() / (inputs.shape[0] * 128 * 128)


            router_labels = torch.argmax(static_targets, dim=1) 
            loss_router = criterion_router(router_logits_128, router_labels)

            # --- 2. Specialized Loss (Huber ç‰ˆ) ---
            mask_0 = (targets_raw < 0.1).float()
            mask_1 = ((targets_raw >= 0.1) & (targets_raw < 10.0)).float()
            mask_2 = (targets_raw >= 10.0).float()
            
            # Expert 0: èƒŒæ™¯ä¸“å®¶
            # Delta = 0.5: ä¸»è¦æ˜¯å¹³æ»‘å»å™ªï¼Œä¸éœ€è¦å¤ªé”åˆ©
            loss_e0 = compute_expert_loss(
                expert_preds_stack[:, 0:1], targets_norm, mask_0, 
                weight_map=None, 
                delta=0.5,       # åå‘ L1 ä¸€ç‚¹ç‚¹ï¼Œä¸ºäº†å½»åº•å½’é›¶
                gdl_coeff=0.0    # èƒŒæ™¯ä¸éœ€è¦å­¦æ¢¯åº¦ï¼Œçœç‚¹è®¡ç®—é‡
            )
            
            # Expert 1: å±‚çŠ¶äº‘ä¸“å®¶ (é™ RMSE çš„ä¸»åŠ›å†›ï¼)
            # Delta = 3.0: å‡ ä¹å…¨éƒ½æ˜¯ MSE è¡Œä¸ºã€‚
            # åªè¦è¯¯å·®å°äº 3.0 (å½’ä¸€åŒ–æ•°å€¼)ï¼Œå®ƒå°±æ˜¯å¹³æ–¹æƒ©ç½šã€‚è¿™ä¼šç–¯ç‹‚å‹ä½ RMSEã€‚
            loss_e1 = compute_expert_loss(
                expert_preds_stack[:, 1:2], targets_norm, mask_1, 
                weight_map=None, 
                delta=0.5,       # ğŸ”¥ æ ¸å¿ƒï¼šå¤§ Delta = MSE æ¨¡å¼ = é™ RMSE
                gdl_coeff=0.5    # ç»™ä¸€ç‚¹ç‚¹æ¢¯åº¦çº¦æŸï¼Œé˜²æ­¢å¤ªå¹³æ»‘
            )
            
            # Expert 2: æš´é›¨ä¸“å®¶ (ä¿ CSI çš„ç‰¹ç§å…µ)
            # Delta = 0.1: å‡ ä¹å…¨éƒ½æ˜¯ L1 è¡Œä¸ºã€‚
            # åªè¦è¯¯å·®è¶…è¿‡ 0.1ï¼Œå°±æ˜¯çº¿æ€§æƒ©ç½šã€‚è¿™å…è®¸å®ƒå¤§èƒ†é¢„æµ‹æå€¼ï¼Œä¸æ€•åç¦»ã€‚
            # energy_weight = torch.pow(targets_raw, 0.5) 
            loss_e2 = compute_expert_loss(
                expert_preds_stack[:, 2:3], targets_norm, mask_2, 
                weight_map=None, 
                delta=3.0,       # ğŸ”¥ æ ¸å¿ƒï¼šå° Delta = L1 æ¨¡å¼ = ä¿æå€¼
                gdl_coeff=5.0    # ğŸ”¥ æ ¸å¿ƒï¼šå¼º GDLï¼Œå¼ºè¿«ç”»å‡ºé”åˆ©è¾¹ç¼˜
            )

            # åˆ«å¿˜äº†æ›´æ–° loss_spec å˜é‡
            loss_spec = (lambda_0 * loss_e0) + (lambda_1 * loss_e1) + (lambda_2 * loss_e2)

            # --- 3. Final & GDL Loss ---
            loss_final = F.huber_loss(final_pred_norm, targets_norm, delta=1.0)
            loss_gdl = criterion_gdl(final_pred_norm, targets_norm)
            
            mask_ex = (targets_raw >= 10.0).float()
            loss_final_ex = 0.0
            if mask_ex.sum() > 0:
                 # åªå¯¹å¼ºé™æ°´åŒºåš MSE
                 loss_final_ex = F.mse_loss(final_pred_norm * mask_ex, targets_norm * mask_ex)

            # 3. ğŸ”¥ 50mm ä¸“é¡¹åŠ æƒ
            # å¦‚æœ batch é‡Œæœ‰ >30mm çš„ç‚¹ï¼Œé¢å¤–åŠ é‡
            mask_30 = (targets_raw >= 30.0).float()
            loss_super_ex = 0.0
            if mask_30.sum() > 0:
                 loss_super_ex = F.mse_loss(final_pred_norm * mask_30, targets_norm * mask_30)

            # 2. ğŸ”¥ èƒŒæ™¯æ•°å€¼æ¸…æ´— (MSE)
            # å¼ºè¿«æœ€ç»ˆè¾“å‡ºä¸º 0
            mask_bg = (targets_raw < 0.1).float()
            loss_bg = 0.0
            if mask_bg.sum() > 0:
                loss_bg = F.mse_loss(final_pred_norm * mask_bg, targets_norm * mask_bg)

            # 3. ğŸ”¥ å¸¦ç¼“å†²åŒºçš„ Router æŠ‘åˆ¶ (Buffer Zone Suppression) ğŸ”¥
            
            # A. å®šä¹‰çœŸå®çš„é›¨åŒº
            rain_mask = (targets_raw >= 0.1).float()
            
            # B. åˆ¶é€ ç¼“å†²åŒºï¼šå¯¹é›¨åŒºè¿›è¡Œè†¨èƒ€ (Dilate)
            # ä½¿ç”¨ MaxPool2d æ¨¡æ‹Ÿè†¨èƒ€ï¼Œkernel=5, padding=2 æ„å‘³ç€å‘å¤–æ‰©å¼  2 ä¸ªåƒç´ 
            # è¿™æ ·ï¼Œé›¨å›¢è¾¹ç¼˜ 2 ä¸ªåƒç´ å†…çš„èƒŒæ™¯éƒ½ä¼šè¢«æ ‡è®°ä¸º 1ï¼Œå—åˆ°ä¿æŠ¤
            rain_mask_dilated = F.max_pool2d(rain_mask, kernel_size=5, stride=1, padding=2)
            
            # C. å®šä¹‰â€œçº¯å‡€èƒŒæ™¯â€ï¼šæ—¢ä¸æ˜¯é›¨ï¼Œä¹Ÿä¸æ˜¯é›¨çš„è¾¹ç¼˜
            # åªæœ‰åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ‰æƒ©ç½š Router çš„èƒ¡ä¹±æ´»è·ƒ
            pure_bg_mask = (1.0 - rain_mask_dilated)
            
            loss_router_bg = 0.0
            if pure_bg_mask.sum() > 0:
                # å–å‡ºéèƒŒæ™¯ä¸“å®¶çš„æ¦‚ç‡
                prob_non_bg = router_probs[:, 1:, :, :].sum(dim=1)
                
                # åªåœ¨çº¯å‡€èƒŒæ™¯åŒºï¼Œä¸”æ¦‚ç‡ > 0.1 çš„åœ°æ–¹è¿›è¡Œæƒ©ç½š
                bad_bg_mask = pure_bg_mask * (prob_non_bg > 0.1).float()
                
                if bad_bg_mask.sum() > 0:
                    loss_router_bg = (prob_non_bg * bad_bg_mask).sum() / (bad_bg_mask.sum() + 1e-6)

            # === ã€æ ¸å¿ƒæ–°å¢ã€‘Dice Loss ===
            # æˆ‘ä»¬ç›´æ¥ä¼˜åŒ– 10mm é˜ˆå€¼çš„ CSI
            # 1. åå½’ä¸€åŒ–é¢„æµ‹å€¼
            preds_raw = denormalize(final_pred_norm, stats['mean'], stats['std'])
            
            # 2. ç”Ÿæˆè½¯æ¦‚ç‡ (ä¸ºäº†å¯å¯¼)
            # æ¸©åº¦è®¾ä¸º 2.0ï¼Œè®©æ¢¯åº¦èƒ½ä¼ å›å»ï¼Œä½†åˆè¶³å¤Ÿé™¡å³­
            pred_prob_10mm = get_heavy_rain_probability(preds_raw, threshold=10.0, temperature=2.0)
            
            # 3. ç”ŸæˆäºŒå€¼åŒ–çœŸå€¼
            target_bin_10mm = (targets_raw >= 10.0).float()
            
            # 4. è®¡ç®— Dice Loss
            loss_dice = criterion_dice(pred_prob_10mm, target_bin_10mm)
            loss_entropy = criterion_entropy(router_probs)

            # --- Total ---
            loss_focal = criterion_focal(pred_prob_10mm, target_bin_10mm)

            # è®¡ç®—é¢„æµ‹æ¦‚ç‡å’ŒçœŸå€¼çš„å…¨å›¾å¹³å‡æ´»è·ƒåº¦ï¼ˆå³é¢ç§¯ï¼‰
            # 1. åŠ¨æ€è®¡ç®—å½’ä¸€åŒ–ç©ºé—´ä¸‹çš„ 10mm é˜ˆå€¼ (æ ¹æ®ä½ çš„ stats è‡ªåŠ¨åŒ¹é…)
            # å…¬å¼: (log1p(10.0) - mean) / std
            norm_threshold_10mm = (np.log1p(10.0) - stats['mean']) / stats['std']

            # 2. è®¡ç®—é¢„æµ‹å’ŒçœŸå€¼åœ¨ 10mm ä»¥ä¸Šçš„å¹³å‡â€œé¢ç§¯â€ (æ¦‚ç‡ç©ºé—´)
            # pred_prob_10mm æ˜¯ç»è¿‡ Sigmoid çš„ (0~1)ï¼Œå…¶å‡å€¼ä»£è¡¨é¢„æŠ¥çš„å¼ºé™æ°´é¢ç§¯å æ¯”
            pred_area = torch.mean(pred_prob_10mm)
            # true_area ä½¿ç”¨å½’ä¸€åŒ–åçš„é˜ˆå€¼åˆ¤æ–­çœŸå€¼çš„å¼ºé™æ°´é¢ç§¯å æ¯”
            true_area = torch.mean((targets_norm > norm_threshold_10mm).float())
            
            # ğŸ”¥ Bias Penalty (ä¿®æ­£ç‰ˆ)
            norm_threshold_10mm = (np.log1p(10.0) - stats['mean']) / stats['std']
            pred_area = torch.mean(pred_prob_10mm)
            true_area = torch.mean((targets_norm > norm_threshold_10mm).float())
            
            # ğŸ”¥ æ ¸å¿ƒè°ƒæ•´ï¼šæ”¾å®½ä¸Šé™è‡³ 1.1 (å…è®¸ 10% çš„è¿‡æŠ¥)
            # åªæœ‰å½“ Bias > 1.1 æ—¶ï¼Œexcess æ‰æ˜¯æ­£æ•°ï¼Œæƒ©ç½šæ‰ç”Ÿæ•ˆ
            # è¿™èƒ½ä¿æŠ¤ PODï¼ŒåŒæ—¶é˜²æ­¢ Bias å¤±æ§ (æ¯”å¦‚é£™åˆ° 1.3)
            target_limit = true_area * 1.15 
            
            excess = torch.relu(pred_area - target_limit)
            bias_penalty = excess / (true_area + 0.01)
            
            loss_fft = fft_loss(final_pred_norm, targets_norm)

            # 4. æ±‡æ€»æ€»æŸå¤±
            total_loss = FINAL_LOSS_WEIGHT * loss_final + \
                         FINAL_EX_WEIGHT * loss_final_ex + \
                         ROUTER_LOSS_WEIGHT * loss_router + \
                         SPECIALIZED_LOSS_WEIGHT * loss_spec + \
                         GDL_LOSS_WEIGHT * loss_gdl + \
                         DICE_WEIGHT * loss_dice + \
                         BIAS_PENALTY_WEIGHT * bias_penalty + \
                         FFT_WEIGHT * loss_fft + \
                         (FINAL_EX_WEIGHT * 2.0) * loss_super_ex 
                         # FINAL_BG_WEIGHT * loss_bg + \
                         # ROUTER_BG_WEIGHT * loss_router_bg # ç»™å¤§æƒé‡ï¼
            
            optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
            optimizer.step()
            
            # === Record ===
            with torch.no_grad():
                meter['total'] += total_loss.item()
                meter['router'] += loss_router.item()
                meter['spec'] += loss_spec.item()
                meter['gdl'] += loss_gdl.item()
                meter['final'] += loss_final.item()
                meter['final_ex'] += loss_final_ex.item() if isinstance(loss_final_ex, torch.Tensor) else loss_final_ex
                meter['dice'] += loss_dice.item()
                meter['bias'] += bias_penalty.item()
                meter['fft'] += loss_fft.item()
                
                probs_avg = router_probs.mean(dim=(0, 2, 3))
                meter['prob_0'] += probs_avg[0].item(); meter['prob_1'] += probs_avg[1].item(); meter['prob_2'] += probs_avg[2].item()
                
                ex_out = denormalize(expert_preds_stack, stats['mean'], stats['std'])
                out_avg = ex_out.mean(dim=(0, 2, 3))
                meter['out_0'] += out_avg[0].item(); meter['out_1'] += out_avg[1].item(); meter['out_2'] += out_avg[2].item()
                steps += 1
            
            loop.set_postfix(loss=total_loss.item())

        # === TensorBoard ===
        for k in meter: meter[k] /= steps
        
        writer.add_scalars('Loss/Components', {'Total': meter['total'], 'Router': meter['router'], 'Spec': meter['spec'], 'GDL': meter['gdl'], 'Final_Ex': meter['final_ex'], 'Dice': meter['dice'], 'Bias': meter['bias'], 'Fft': meter['fft']},  epoch)
        writer.add_scalars('Analysis/Router_Prob', {'Exp0': meter['prob_0'], 'Exp1': meter['prob_1'], 'Exp2': meter['prob_2']}, epoch)
        writer.add_scalars('Analysis/Expert_Out', {'Exp0': meter['out_0'], 'Exp1': meter['out_1'], 'Exp2': meter['out_2']}, epoch)

        # Validation (å¸¦ CSI å’Œ å¯è§†åŒ–)
        # --- Validation ---
        val_metrics, viz_tensors = validate_epoch_detailed(model, val_loader, stats, DEVICE)
        
        # è®°å½•åˆ° TensorBoard
        writer.add_scalar('Metric/Overall_RMSE', val_metrics['rmse'], epoch)
        writer.add_scalar('Metric/Extreme_RMSE', val_metrics['ext_rmse'], epoch)
        writer.add_scalar('Metric/CSI_10', val_metrics['csi'], epoch)
        writer.add_scalar('Metric/POD_10', val_metrics['pod'], epoch)
        writer.add_scalar('Metric/FAR_10', val_metrics['far'], epoch)
        writer.add_scalar('Metric/BIAS_10', val_metrics['bias'], epoch)
        
        # æ§åˆ¶å°æ‰“å°è¾“å‡ºï¼Œæ–¹ä¾¿è§‚å¯Ÿè¶‹åŠ¿
        print(f"Ep {epoch+1}: RMSE={val_metrics['rmse']:.4f} | CSI={val_metrics['csi']:.4f} | "
              f"POD={val_metrics['pod']:.4f} | FAR={val_metrics['far']:.4f} | BIAS={val_metrics['bias']:.4f}")
        
        # 2. å¯è§†åŒ–
        if viz_tensors:
            inp, pred, gt, r_probs = viz_tensors
            img_input = denormalize(inp[-1].unsqueeze(0), stats['mean'], stats['std'])
            vmax = 20.0
            grid_img = torchvision.utils.make_grid([colorize(img_input, vmax=vmax), colorize(pred, vmax=vmax), colorize(gt, vmax=vmax)], nrow=3)
            writer.add_image('Vis/Comparison', grid_img, epoch)
            grid_router = torchvision.utils.make_grid(r_probs, nrow=3, normalize=True)
            writer.add_image('Vis/Router', grid_router, epoch)
        
        # 3. æ§åˆ¶å°æ‰“å° (ä¿®å¤åŸæ¥çš„ NameError)
        print(f"Ep {epoch+1}: Loss={meter['total']:.4f} | RMSE={val_metrics['rmse']:.4f} | "
              f"CSI={val_metrics['csi']:.4f} | POD={val_metrics['pod']:.4f} | "
              f"FAR={val_metrics['far']:.4f} | BIAS={val_metrics['bias']:.4f}")
        
        # 4. æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_metrics['rmse'])

        # 5. ä¿å­˜æœ€ä¼˜æ¨¡å‹
        # åˆ¤æ–­ Extreme RMSE
        if val_metrics['ext_rmse'] < best_extreme_rmse:
            best_extreme_rmse = val_metrics['ext_rmse']
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_extreme_model.pth'))
            print(f"ğŸ”¥ Saved Best Extreme Model: {best_extreme_rmse:.4f}")

        # åˆ¤æ–­ CSI
        if val_metrics['csi'] > best_csi:
            best_csi = val_metrics['csi']
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_csi_model.pth'))
            print(f"ğŸŒŠ Saved Best CSI Model: {best_csi:.4f}")

    writer.close()
    
    # 1. æµ‹è¯• RMSE æœ€å¥½çš„æ¨¡å‹
    if os.path.exists(os.path.join(SAVE_DIR, 'best_extreme_model.pth')):
        print(">>> Testing Best Extreme RMSE Model...")
        model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'best_extreme_model.pth')))
        # æ³¨æ„è¿™é‡Œè¦æŠŠæ‰€æœ‰å‚æ•°ä¼ è¿›å»
        final_test_pipeline(model, test_loader, stats, DEVICE, SAVE_DIR, prefix='rmse_best')

    # 2. æµ‹è¯• CSI æœ€å¥½çš„æ¨¡å‹
    if os.path.exists(os.path.join(SAVE_DIR, 'best_csi_model.pth')):
        print(">>> Testing Best CSI Model...")
        model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'best_csi_model.pth')))
        # æ³¨æ„è¿™é‡Œè¦æŠŠæ‰€æœ‰å‚æ•°ä¼ è¿›å»
        final_test_pipeline(model, test_loader, stats, DEVICE, SAVE_DIR, prefix='csi_best')


if __name__ == "__main__":
    main()