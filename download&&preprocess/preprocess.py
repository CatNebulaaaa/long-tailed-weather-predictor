import os
import numpy as np
import xarray as xr
import json
import shutil
from tqdm import tqdm
import cv2

# ================= é…ç½®åŒº =================
INPUT_FILE = '/root/autodl-tmp/merged_data.nc' 
OUTPUT_ZARR_DIR = '/root/autodl-tmp/processed_america_data_with_raw_and_labels.zarr' # å»ºè®®ç”¨æ–°åå­—
STATS_FILE = '/root/autodl-tmp/america_normalization_stats.json'
TARGET_SIZE = 128
TRAIN_RATIO = 0.7
VAL_RATIO = 0.1
MAX_PRECIP = 200.0 
# ä¼˜åŒ–è¯»å–çš„å…³é”®ï¼šä¸ºæ¯ä¸ªå˜é‡è®¾ç½®ç‹¬ç«‹çš„ã€ä¼˜åŒ–çš„ chunks
CHUNKS_RAW = {'time': 100, 'lat': TARGET_SIZE, 'lon': TARGET_SIZE}
CHUNKS_LABELS = {'time': 100, 'regime': 3, 'lat': TARGET_SIZE, 'lon': TARGET_SIZE}

# V2 æ ‡ç­¾çš„ç‰©ç†å‚æ•°
THRESHOLDS = {'rain': 0.1, 'convective': 15.0}
AREA_LIMITS = {'convective_max': 1000, 'stratiform_min': 1000}
# ==========================================

def generate_region_labels_for_preprocess(gt_precip_np):
    """
    ä¸€ä¸ªç®€åŒ–çš„ã€åªæ¥å— numpy è¾“å…¥çš„ V2 æ ‡ç­¾ç”Ÿæˆå‡½æ•°ã€‚
    """
    H, W = gt_precip_np.shape
    
    # === æ ¸å¿ƒä¼˜åŒ–ï¼šå°† float32 ä¿®æ”¹ä¸º uint8 ===
    # uint8 ä½¿ç”¨ 1 ä¸ªå­—èŠ‚ï¼Œè€Œ float32 ä½¿ç”¨ 4 ä¸ªå­—èŠ‚ã€‚
    # å¯¹äºåªæœ‰ 0 å’Œ 1 çš„æ©ç ï¼Œuint8 è¶³å¤Ÿäº†ï¼Œå¹¶ä¸”å¯ä»¥èŠ‚çœ 75% çš„ç©ºé—´ã€‚
    hard_masks = np.zeros((3, H, W), dtype=np.uint8) # <--- åœ¨è¿™é‡Œä¿®æ”¹ï¼
    # ==========================================

    # 1. èƒŒæ™¯ (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜)
    hard_masks[0] = (gt_precip_np < THRESHOLDS['rain'])
    
    # 2. è¿é€šåŸŸåˆ†æ (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜)
    rain_mask_np = (gt_precip_np >= THRESHOLDS['rain']).astype(np.uint8)
    num_labels, labels_map, stats, _ = cv2.connectedComponentsWithStats(rain_mask_np, 8)

    if num_labels > 1:
        for label_id in range(1, num_labels):
            area = stats[label_id, cv2.CC_STAT_AREA]
            component_mask = (labels_map == label_id)
            
            if not np.any(component_mask):
                continue
            max_intensity = gt_precip_np[component_mask].max()
            
            is_convective = (area < AREA_LIMITS['convective_max'] and 
                             max_intensity > THRESHOLDS['convective'])
            is_stratiform = (area >= AREA_LIMITS['stratiform_min'])
            
            if is_convective:
                hard_masks[2][component_mask] = 1
            elif is_stratiform:
                hard_masks[1][component_mask] = 1
            else:
                hard_masks[1][component_mask] = 1
                
    return hard_masks


def process_and_save_group(data_raw, group_name, stats):
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå¤„ç†å•ä¸ªæ•°æ®é›†åˆ†ç»„å¹¶ä¿å­˜ã€‚"""
    print(f"\n--- å¼€å§‹å¤„ç†åˆ†ç»„: {group_name} ---")
    
    # 1. è®¡ç®—å½’ä¸€åŒ–æ•°æ® (æ‡’åŠ è½½)
    print("è®¡ç®—å½’ä¸€åŒ–æ•°æ®...")
    data_norm = (np.log1p(data_raw) - stats['mean']) / (stats['std'] + 1e-6)
    
    # 2. è®¡ç®—æ ‡ç­¾ (ç«‹å³è®¡ç®—ï¼Œå› ä¸ºéœ€è¦éå†)
    print("è®¡ç®—åŒºåŸŸæ ‡ç­¾...")
    # ä½¿ç”¨ .load() å°† dask array åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œä»¥ä¾¿ numpy å¯ä»¥å¤„ç†
    # è¿™ä¸€æ­¥ä¼šæ¶ˆè€—å†…å­˜ï¼Œä½†å¯¹äºæ ‡ç­¾è®¡ç®—æ˜¯å¿…è¦çš„
    raw_numpy = data_raw.load().values 
    
    labels_list = [generate_region_labels_for_preprocess(frame) for frame in tqdm(raw_numpy, desc=f"ç”Ÿæˆæ ‡ç­¾ ({group_name})")]
    labels_array = np.stack(labels_list, axis=0)
    
    # 3. åˆ›å»ºåŒ…å«æ‰€æœ‰å˜é‡çš„æœ€ç»ˆ Dataset
    final_ds = xr.Dataset({
        'precipitation_norm': data_norm,
        'precipitation_raw': data_raw,
        'region_labels': xr.DataArray(
            labels_array,
            dims=('time', 'regime', 'lat', 'lon'),
            coords={'regime': [0, 1, 2]} # åªéœ€è¦æä¾›æ–°çš„åæ ‡
        )
    })
    
    # 4. è®¾ç½®åˆ†å—å¹¶ä¿å­˜
    # ç¡®ä¿æ—¶é—´ã€ç»çº¬åº¦åæ ‡ä¹Ÿè¢«æ­£ç¡®å†™å…¥
    final_ds = final_ds.assign_coords({
        'time': data_raw.time,
        'lat': data_raw.lat,
        'lon': data_raw.lon
    })

    print("è®¾ç½®åˆ†å—å¹¶ä¿å­˜åˆ° Zarr...")
    final_ds = final_ds.chunk({'time': 100}) # ç»Ÿä¸€è®¾ç½® time chunk
    
    mode = 'w' if group_name == 'train' else 'a'
    final_ds.to_zarr(OUTPUT_ZARR_DIR, group=group_name, mode=mode, consolidated=True)
    print(f"âœ… åˆ†ç»„ '{group_name}' ä¿å­˜æˆåŠŸ!")


def main():
    if os.path.exists(OUTPUT_ZARR_DIR):
        print(f"æ£€æµ‹åˆ°æ—§ç›®å½• {OUTPUT_ZARR_DIR}ï¼Œæ­£åœ¨åˆ é™¤...")
        shutil.rmtree(OUTPUT_ZARR_DIR)

    print(f"æ­£åœ¨æ‰“å¼€æ•°æ®æ–‡ä»¶å¤¹: /root/autodl-tmp/GPM_FINAL/")
    # ä½¿ç”¨ open_mfdataset è‡ªåŠ¨é€»è¾‘åˆå¹¶
    # chunks è®¾ç½®å»ºè®®ä¿æŒï¼Œæ–¹ä¾¿åç»­ Dask å¹¶è¡Œè®¡ç®—
    ds = xr.open_mfdataset('/root/autodl-tmp/GPM_FINAL/*.nc', 
                           combine='by_coords', 
                           chunks={'time': 1000})
    da = ds['precipitation']

    print("1. æ•°æ®æ¸…æ´—ä¸è£å‰ª...")
    da = da.isel(lat=slice(0, TARGET_SIZE), lon=slice(0, TARGET_SIZE))
    da = da.sortby('time')
    da = da.where(da >= 0, 0).clip(max=MAX_PRECIP)

    print("2. æ•°æ®é›†åˆ’åˆ†...")
    n_samples = da.sizes['time']
    n_train = int(n_samples * TRAIN_RATIO)
    n_val = int(n_samples * VAL_RATIO)
    train_raw = da.isel(time=slice(0, n_train))
    val_raw = da.isel(time=slice(n_train, n_train + n_val))
    test_raw = da.isel(time=slice(n_train + n_val, None))

    print("3. è®¡ç®—è®­ç»ƒé›†ç»Ÿè®¡é‡...")
    train_log = np.log1p(train_raw)
    mean_val = train_log.mean().compute().item()
    std_val = train_log.std().compute().item()
    stats = {"mean": mean_val, "std": std_val}
    with open(STATS_FILE, 'w') as f: json.dump(stats, f, indent=4)
    print(f"   è®¡ç®—å®Œæˆ: Mean={mean_val:.4f}, Std={std_val:.4f}")

    # 4. ä¾æ¬¡å¤„ç†å¹¶ä¿å­˜æ¯ä¸ªåˆ†ç»„
    process_and_save_group(train_raw, 'train', stats)
    process_and_save_group(val_raw, 'val', stats)
    process_and_save_group(test_raw, 'test', stats)

    print(f"\nğŸ‰ğŸ‰ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ {OUTPUT_ZARR_DIR}")

if __name__ == "__main__":
    main()